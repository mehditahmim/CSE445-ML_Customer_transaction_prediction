{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-afdd118cf1cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m#All the training and testing and query testing is done here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import  StratifiedKFold\n",
    "#4\n",
    "#You can see this segment\n",
    "#All the training and testing and query testing is done here\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "import random\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.backends.backend_pdf\n",
    "from random import randint\n",
    "import os\n",
    "import os.path\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import datetime\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(split, x, model, opt, labels=None, B=32):\n",
    "        torch.set_grad_enabled(split=='train') \n",
    "        model.train() if split == 'train' else model.eval()\n",
    "    \n",
    "        N,D = x.size()\n",
    "        nsteps = int(N/B) + (N%B !=0)\n",
    "        output = []\n",
    "        \n",
    "        for step in range(nsteps):\n",
    "            lb = step*B\n",
    "            ub = lb+ min(B, N-lb)\n",
    "            xb = Variable(x[lb:ub])\n",
    "            \n",
    "            \n",
    "            if split==\"train\":\n",
    "#                 w = torch.from_numpy(np.array([10.0, 1.0])).cuda().float()\n",
    "                xbhat = model(xb)\n",
    "                predictions = Variable(labels[lb:ub].reshape(-1,1))\n",
    "                loss = F.binary_cross_entropy_with_logits(xbhat,predictions, size_average=True) / (ub-lb)\n",
    "                lossf = loss.data.item()\n",
    "                output.append(lossf)\n",
    "            else:\n",
    "                xbhat = model(xb)\n",
    "                output.extend(xbhat.cpu().detach().numpy().flatten().tolist())\n",
    "                \n",
    "            if split == 'train':\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                \n",
    "        return output\n",
    "    \n",
    "def train_model( x_train, labels, hidden_list, max_epoch, B):\n",
    "    file_name=\"nn_model\"\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "    xtr = torch.from_numpy(x_train).cuda().float()\n",
    "    tlabels = torch.from_numpy(labels).cuda().float()\n",
    "\n",
    "\n",
    "    hs = [xtr.size(1)] + hidden_list +[1]\n",
    "    net=[]\n",
    "    for h0, h1 in zip(hs, hs[1:]):\n",
    "        net.extend([\n",
    "            nn.Linear(h0, h1),\n",
    "            nn.ReLU()\n",
    "        ])\n",
    "\n",
    "    net.pop()  \n",
    "    net.append(nn.Sigmoid())\n",
    "\n",
    "    model = nn.Sequential(*net)\n",
    "    model.cuda()\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), 1e-3, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=45, gamma=0.1)\n",
    "    current_loss = 1000000.0\n",
    "    firstTimeFlag = True\n",
    "    \n",
    "    for epoch in range(1,1+max_epoch):\n",
    "        \n",
    "        scheduler.step(epoch)\n",
    "        lssf = run_epoch('train', xtr, model, opt, tlabels, B)\n",
    "        \n",
    "        mean_loss = np.mean(lssf)\n",
    "        \n",
    "        if epoch % 10 ==0:\n",
    "            print(\"epoch\",epoch, \"loss\", mean_loss)\n",
    "            \n",
    "        if mean_loss < current_loss or firstTimeFlag:\n",
    "            current_loss=mean_loss\n",
    "            torch.save(model, file_name)\n",
    "            firstTimeFlag=False\n",
    "            \n",
    "    return model, test_single_query(model, x_train)\n",
    "    \n",
    "def test_single_query(model, xte):\n",
    "    file_name=\"nn_model\"\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "    \n",
    "    xte=torch.from_numpy(xte).cuda().float()\n",
    "    opt = torch.optim.Adam(model.parameters(), 1e-3, weight_decay=1e-4)\n",
    "    result_list = run_epoch('test',xte, model, opt,B=5000)\n",
    "    \n",
    "    return np.array(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input():\n",
    "    #Basic Input\n",
    "    data = pd.read_csv(\"train.csv\", header=0) \n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    np_data= data.values\n",
    "    labels = np_data[:,1].astype('int')\n",
    "    train= np_data[:,2:].astype(np.float32)\n",
    "    print(\"Train Shape\", train.shape)\n",
    "    test = pd.read_csv(\"test.csv\", header=0).values \n",
    "    ids = test[:,0]\n",
    "    test= test[:,1:].astype(np.float32)\n",
    "    print(\"Test Shape\", test.shape)\n",
    "    return train,labels, test, ids\n",
    "\n",
    "def read_input_balanced():\n",
    "    #Basic Input\n",
    "    data = pd.read_csv(\"train.csv\", header=0) \n",
    "    data1 = data[data[\"target\"]==0]\n",
    "    data2 = data[data[\"target\"]==1]\n",
    "\n",
    "    frac= float(data1.shape[0])/data2.shape[0]\n",
    "    data2 = data2.sample(data1.shape[0],replace=True).reset_index(drop=True)\n",
    "    data_new = pd.concat([data1,data2], ignore_index=True)\n",
    "    data_new = data_new.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    np_data= data_new.values\n",
    "    labels = np_data[:,1].astype('int')\n",
    "    train= np_data[:,2:].astype(np.float32)\n",
    "    print(\"Train Shape\", train.shape)\n",
    "    test = pd.read_csv(\"test.csv\", header=0).values \n",
    "    ids = test[:,0]\n",
    "    test= test[:,1:].astype(np.float32)\n",
    "    print(\"Test Shape\", test.shape)\n",
    "    return train,labels, test, ids\n",
    "\n",
    "def classify(hidden_list,X_train, X_test, y_train, y_test):\n",
    "    print(y_train.shape)\n",
    "    model,predictions = train_model(X_train, y_train, hidden_list, 100, 256)\n",
    "    \n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, predictions)\n",
    "    roc_auc_tr = auc(false_positive_rate, true_positive_rate)\n",
    "    \n",
    "    \n",
    "    pred_test = test_single_query(model, X_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, pred_test)\n",
    "    roc_auc_test = auc(false_positive_rate, true_positive_rate)\n",
    "    return roc_auc_tr, roc_auc_test, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_train, org_labels, org_test,ids = read_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(359804, 200) (200000, 200)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(559804, 200)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(org_train.shape, org_test.shape)\n",
    "x=np.concatenate((org_train, org_test), axis=0)\n",
    "x.shape                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188969, 200)\n"
     ]
    }
   ],
   "source": [
    "def normalize(train):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train)\n",
    "\n",
    "    return scaler\n",
    "\n",
    "def append_bin_features(train):\n",
    "    \n",
    "    min_col = np.min(train, axis=1).reshape((-1,1))\n",
    "    max_col = np.max(train, axis=1).reshape((-1,1))\n",
    "    mean_col = np.mean(train, axis=1).reshape((-1,1))\n",
    "    std_col = np.std(train, axis=1).reshape((-1,1))\n",
    "    sum_col = np.sum(train, axis=1).reshape((-1,1))\n",
    "    med_col = np.median(train, axis=1).reshape((-1,1))\n",
    "    skew_col = scipy.stats.skew(train, axis=1).reshape((-1,1))\n",
    "    kurtosis_col = scipy.stats.kurtosis(train, axis=1).reshape((-1,1))\n",
    "\n",
    "    new_train = train.copy()\n",
    "    \n",
    "    new_train=np.column_stack((new_train, min_col))\n",
    "    new_train=np.column_stack((new_train, max_col))\n",
    "    new_train=np.column_stack((new_train, mean_col))\n",
    "    new_train=np.column_stack((new_train, std_col))\n",
    "    new_train=np.column_stack((new_train, sum_col))\n",
    "    new_train=np.column_stack((new_train, med_col))\n",
    "    new_train=np.column_stack((new_train, skew_col))\n",
    "    new_train=np.column_stack((new_train, kurtosis_col))\n",
    "    \n",
    "    lst= [0.01,0.05,0.10,0.25,0.50,0.60, 0.70, 0.80, 0.9, 0.95]\n",
    "    for l in lst:\n",
    "        quintile_col = np.quantile(train,l, axis=1)\n",
    "        new_train=np.column_stack((new_train, med_col))\n",
    "    \n",
    "    total_features= new_train.shape[0]\n",
    "    \n",
    "    \n",
    "    bin_train = train.copy()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(bin_train)\n",
    "    min_max_train = np.round(scaler.transform(bin_train) * 10.0)\n",
    "    \n",
    "    scaler2 = MinMaxScaler()\n",
    "    scaler2.fit(min_max_train)\n",
    "    min_max_train = scaler2.transform(min_max_train)\n",
    "    \n",
    "    new_train=np.column_stack((new_train, min_max_train))\n",
    "    \n",
    "    \n",
    "    return new_train\n",
    "\n",
    "def remove_outlier(train, labels):\n",
    "    z = np.abs(stats.zscore(train))\n",
    "    threshold = 3\n",
    "    x,y=np.where(z > 3)\n",
    "    lst= np.unique(x)\n",
    "    all_indices = [i for i in range(0, train.shape[0])]\n",
    "    non_outlier_indices = list(set(all_indices)-set(lst))\n",
    "    train2 = train[non_outlier_indices,:]\n",
    "    labels2 = labels[non_outlier_indices]\n",
    "    return train2, labels2\n",
    "\n",
    "train, labels = remove_outlier(org_train, org_labels)\n",
    "print(train.shape)\n",
    "\n",
    "train_test = append_bin_features(np.concatenate((train, org_test), axis=0))\n",
    "train = train_test [:train.shape[0],:]\n",
    "test = train_test [train.shape[0]:,:]\n",
    "\n",
    "# train = org_train\n",
    "# test = org_test\n",
    "\n",
    "scaler = normalize( np.concatenate((train, test), axis=0))\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(171444,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbxfalcon/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 loss 0.013149777710882585\n",
      "epoch 20 loss 0.01313916825508671\n",
      "epoch 30 loss 0.01313311215629106\n",
      "epoch 40 loss 0.01312863991192695\n",
      "epoch 50 loss 0.01310882315095236\n",
      "epoch 60 loss 0.013108781709877858\n",
      "epoch 70 loss 0.013108705750096645\n",
      "epoch 80 loss 0.013108655018036935\n",
      "epoch 90 loss 0.013105448221426402\n",
      "epoch 100 loss 0.013105255511325242\n",
      "0.8651517186558089 0.8633411046854615\n",
      "(171445,)\n",
      "epoch 10 loss 0.013160333761583959\n",
      "epoch 20 loss 0.013149344986443644\n"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=2,shuffle = False, random_state=546)\n",
    "\n",
    "tr_auc_list = []\n",
    "vd_auc_list=[]\n",
    "for train_index, test_index in kf.split(train, labels):\n",
    "    X_train, X_test = train[train_index,:], train[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    tr_auc,test_auc,classifier = classify([16],X_train, X_test,y_train , y_test)\n",
    "    \n",
    "    tr_auc_list.append(tr_auc)\n",
    "    vd_auc_list.append(test_auc)\n",
    "    \n",
    "    print(tr_auc, test_auc)\n",
    "print(np.mean(tr_auc_list), np.mean(vd_auc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188969,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dbxfalcon/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10 loss 0.0028033815314854635\n",
      "epoch 20 loss 0.002803306481252766\n",
      "epoch 30 loss 0.0028033133565305815\n",
      "epoch 40 loss 0.002803312748461377\n",
      "epoch 50 loss 0.002803318145154334\n",
      "epoch 60 loss 0.002803298880072646\n",
      "epoch 70 loss 0.0028032989837279663\n",
      "epoch 80 loss 0.0028032996841102675\n",
      "epoch 90 loss 0.0028034836316609777\n",
      "epoch 100 loss 0.0028032564510199393\n"
     ]
    }
   ],
   "source": [
    "tr_auc,test_auc,classifier = classify([200,50],train, train, labels , labels)\n",
    "test_predictions = test_single_query(classifier, test)\n",
    "columns=['ID_code','target']\n",
    "query_output = pd.DataFrame(columns=columns)\n",
    "query_output['ID_code']= ids\n",
    "query_output['target']=test_predictions\n",
    "query_output.to_csv('output_nn.csv', index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6753856967238385 0.6753856967238385\n"
     ]
    }
   ],
   "source": [
    "print(tr_auc, test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StratifiedKFold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3036a732db9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m546\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtr_auc_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvd_auc_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StratifiedKFold' is not defined"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=2,shuffle = False, random_state=546)\n",
    "\n",
    "tr_auc_list = []\n",
    "vd_auc_list=[]\n",
    "for train_index, test_index in kf.split(train, labels):\n",
    "    X_train, X_test = train[train_index,:], train[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    tr_auc,test_auc,classifier = classify([16],X_train, X_test,y_train , y_test)\n",
    "    \n",
    "    tr_auc_list.append(tr_auc)\n",
    "    vd_auc_list.append(test_auc)\n",
    "    \n",
    "    print(tr_auc, test_auc)\n",
    "print(np.mean(tr_auc_list), np.mean(vd_auc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\thm_m\\\\Desktop\\\\ML\\\\ML'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\thm_m\\\\Desktop\\\\ML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
