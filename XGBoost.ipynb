{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input():\n",
    "    data = pd.read_csv('train.csv', header=0) \n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    np_data= data.values\n",
    "    labels = np_data[:,1].astype('int')\n",
    "    train= np_data[:,2:].astype(np.float32)\n",
    "    print(\"Train Shape\", train.shape)\n",
    "    test = pd.read_csv(\"test.csv\", header=0).values \n",
    "    ids = test[:,0]\n",
    "    test= test[:,1:].astype(np.float32)\n",
    "    print(\"Test Shape\", test.shape)\n",
    "    return train,labels, test, ids\n",
    "\n",
    "def remove_outlier(train, labels):\n",
    "    z = np.abs(stats.zscore(train))\n",
    "    threshold = 3\n",
    "    x,y=np.where(z > 3)\n",
    "    lst= np.unique(x)\n",
    "    all_indices = [i for i in range(0, train.shape[0])]\n",
    "    non_outlier_indices = list(set(all_indices)-set(lst))\n",
    "    train2 = train[non_outlier_indices,:]\n",
    "    labels2 = labels[non_outlier_indices]\n",
    "    return train2, labels2\n",
    "\n",
    "def normalize(train):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train)\n",
    "\n",
    "    return scaler\n",
    "def min_max_normalize(train):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train)\n",
    "    return scaler\n",
    "\n",
    "#Append few extra features/columns\n",
    "def append_features(train):\n",
    "    min_col = np.min(train, axis=1).reshape((-1,1))\n",
    "    max_col = np.max(train, axis=1).reshape((-1,1))\n",
    "    mean_col = np.mean(train, axis=1).reshape((-1,1))\n",
    "    std_col = np.std(train, axis=1).reshape((-1,1))\n",
    "    sum_col = np.sum(train, axis=1).reshape((-1,1))\n",
    "    med_col = np.median(train, axis=1).reshape((-1,1))\n",
    "\n",
    "    new_train = train.copy()\n",
    "    \n",
    "    new_train=np.column_stack((new_train, min_col))\n",
    "    new_train=np.column_stack((new_train, max_col))\n",
    "    new_train=np.column_stack((new_train, mean_col))\n",
    "    new_train=np.column_stack((new_train, std_col))\n",
    "    new_train=np.column_stack((new_train, sum_col))\n",
    "    new_train=np.column_stack((new_train, med_col))\n",
    "    \n",
    "   # lst= [0.01,0.05,0.10,0.25,0.50,0.60, 0.70, 0.80, 0.9, 0.95]\n",
    "    #for l in lst:\n",
    "     #   quintile_col = np.quantile(train,l, axis=1)\n",
    "      #  new_train=np.column_stack((new_train, med_col))\n",
    "    \n",
    "    total_features= new_train.shape[0]\n",
    "    \n",
    "#     new_train_2 = new_train**2\n",
    "#     new_train_3 = new_train**2\n",
    "    \n",
    "#     new_train=np.column_stack((new_train, new_train_2))\n",
    "#     new_train=np.column_stack((new_train, new_train_3))\n",
    "        \n",
    "    return new_train\n",
    "#Append few extra features/columns\n",
    "def append_bin_features(train):\n",
    "    \n",
    "    min_col = np.min(train, axis=1).reshape((-1,1))\n",
    "    max_col = np.max(train, axis=1).reshape((-1,1))\n",
    "    mean_col = np.mean(train, axis=1).reshape((-1,1))\n",
    "    std_col = np.std(train, axis=1).reshape((-1,1))\n",
    "    sum_col = np.sum(train, axis=1).reshape((-1,1))\n",
    "    med_col = np.median(train, axis=1).reshape((-1,1))\n",
    "    skew_col = scipy.stats.skew(train, axis=1).reshape((-1,1))\n",
    "    kurtosis_col = scipy.stats.kurtosis(train, axis=1).reshape((-1,1))\n",
    "\n",
    "    new_train = train.copy()\n",
    "    \n",
    "    new_train=np.column_stack((new_train, min_col))\n",
    "    new_train=np.column_stack((new_train, max_col))\n",
    "    new_train=np.column_stack((new_train, mean_col))\n",
    "    new_train=np.column_stack((new_train, std_col))\n",
    "    new_train=np.column_stack((new_train, sum_col))\n",
    "    new_train=np.column_stack((new_train, med_col))\n",
    "    new_train=np.column_stack((new_train, skew_col))\n",
    "    new_train=np.column_stack((new_train, kurtosis_col))\n",
    "    \n",
    "    \n",
    "    \n",
    "    bin_train = train.copy()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(bin_train)\n",
    "    min_max_train = np.round(scaler.transform(bin_train) * 10.0)\n",
    "    \n",
    "    scaler2 = MinMaxScaler()\n",
    "    scaler2.fit(min_max_train)\n",
    "    min_max_train = scaler2.transform(min_max_train)\n",
    "    \n",
    "    new_train=np.column_stack((new_train, min_max_train))\n",
    "    \n",
    "    return new_train\n",
    "\n",
    "def classify(X_train, X_test, y_train, y_test, nest = 500, max_depth=3):\n",
    "#     scale_pos_weight = float(np.sum(y_train == 1))/y_train.shape[0]\n",
    "    scale_pos_weight = 10\n",
    "    print(\"Nest\",nest)\n",
    "    classifier = XGBClassifier(random_state=42,\n",
    "                      scale_pos_weight=scale_pos_weight,\n",
    "                      max_depth=max_depth,\n",
    "                      nestimator=nest, \n",
    "                      learning_rate=0.1,  \n",
    "                      colsample_bytree = 0.4,\n",
    "                      subsample = 0.8,\n",
    "                      objective='binary:logistic')\n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    \n",
    "    eval_metric = [\"auc\"]\n",
    "    classifier.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set,verbose= False)\n",
    "    \n",
    "    predictions = classifier.predict(X_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, predictions)\n",
    "    roc_auc_tr = auc(false_positive_rate, true_positive_rate)\n",
    "    pred_test = classifier.predict(X_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, pred_test)\n",
    "    roc_auc_test = auc(false_positive_rate, true_positive_rate)\n",
    "    return roc_auc_tr, roc_auc_test, classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape (200000, 200)\n",
      "Test Shape (200000, 200)\n"
     ]
    }
   ],
   "source": [
    "org_train, org_labels, org_test,ids = read_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nest 25\n",
      "Training and Test AUC: 0.7731611742299974 0.7731611742299974\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'output_xgboost.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ac62cac7c3f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mquery_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID_code'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mquery_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mquery_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output_xgboost.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquery_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ID_code'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda1\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   1743\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 1745\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda1\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[0;32m    155\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m                                      compression=self.compression)\n\u001b[0m\u001b[0;32m    157\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda1\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;31m# Python 3 and encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;31m# Python 3 and no explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'output_xgboost.csv'"
     ]
    }
   ],
   "source": [
    "tr_auc,test_auc,classifier= classify(org_train, org_train, org_labels, org_labels)\n",
    "print(\"Training and Test AUC:\", tr_auc, test_auc)\n",
    "\n",
    "test_predictions = classifier.predict(org_test)\n",
    "columns=['ID_code','target']\n",
    "query_output = pd.DataFrame(columns=columns)\n",
    "query_output['ID_code']= ids\n",
    "query_output['target']=test_predictions\n",
    "query_output.to_csv('output_xgboost.csv', index=False,sep=',')   \n",
    "print(query_output['target'],query_output['ID_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.20)\n",
    "n_estimators = [1, 16, 32, 64, 100,500]\n",
    "train_results=[]\n",
    "test_results=[]\n",
    "for nest in n_estimators:\n",
    "    tr_auc,test_auc,classifier = classify(X_train, X_test, y_train, y_test, nest=nest)\n",
    "    train_results.append(tr_auc)\n",
    "    test_results.append(test_auc)\n",
    "    print(\"Number of Trees\", nest, tr_auc, test_auc)\n",
    "\n",
    "\n",
    "line1, = plt.plot(n_estimators, train_results, 'b', label='Train AUC')\n",
    "line2, = plt.plot(n_estimators, test_results, 'r', label='Validation AUC')\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\thm_m\\\\Desktop\\\\ML\\\\ML'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\n",
    "'C:\\\\Users\\\\thm_m\\\\Desktop\\\\ML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\thm_m\\\\Desktop\\\\ML'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
