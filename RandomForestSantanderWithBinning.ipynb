{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Santander by Random Forest </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input():\n",
    "    #Basic Input\n",
    "    data = pd.read_csv(\"train.csv\", header=0) \n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    #get the 2d array from pandas\n",
    "    np_data= data.values\n",
    "    \n",
    "    labels = np_data[:,1].astype('int')\n",
    "    train= np_data[:,2:].astype(np.float32)\n",
    "    print(\"Train Shape\", train.shape)\n",
    "    test = pd.read_csv(\"test.csv\", header=0).values \n",
    "    ids = test[:,0]\n",
    "    test= test[:,1:].astype(np.float32)\n",
    "    print(\"Test Shape\", test.shape)\n",
    "    return train,labels, test, ids\n",
    "\n",
    "\n",
    "\n",
    "def remove_outlier(train, labels):\n",
    "    z = np.abs(stats.zscore(train))\n",
    "    threshold = 3\n",
    "    x,y=np.where(z > 3)\n",
    "    lst= np.unique(x)\n",
    "    all_indices = [i for i in range(0, train.shape[0])]\n",
    "    non_outlier_indices = list(set(all_indices)-set(lst))\n",
    "    train2 = train[non_outlier_indices,:]\n",
    "    labels2 = labels[non_outlier_indices]\n",
    "    return train2, labels2\n",
    "\n",
    "def normalize(train):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train)\n",
    "\n",
    "    return scaler\n",
    "def min_max_normalize(train):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train)\n",
    "    return scaler\n",
    "\n",
    "#Append few extra features/columns\n",
    "def append_features(train):\n",
    "    min_col = np.min(train, axis=1).reshape((-1,1))\n",
    "    max_col = np.max(train, axis=1).reshape((-1,1))\n",
    "    mean_col = np.mean(train, axis=1).reshape((-1,1))\n",
    "    std_col = np.std(train, axis=1).reshape((-1,1))\n",
    "    sum_col = np.sum(train, axis=1).reshape((-1,1))\n",
    "    med_col = np.median(train, axis=1).reshape((-1,1))\n",
    "    skew_col = scipy.stats.skew(train, axis=1).reshape((-1,1))\n",
    "    kurtosis_col = scipy.stats.kurtosis(train, axis=1).reshape((-1,1))\n",
    "    \n",
    "\n",
    "    new_train = train.copy()\n",
    "    \n",
    "    new_train=np.column_stack((new_train, min_col))\n",
    "    new_train=np.column_stack((new_train, max_col))\n",
    "    new_train=np.column_stack((new_train, mean_col))\n",
    "    new_train=np.column_stack((new_train, std_col))\n",
    "    new_train=np.column_stack((new_train, sum_col))\n",
    "    new_train=np.column_stack((new_train, med_col))\n",
    "    new_train=np.column_stack((new_train, skew_col))\n",
    "    new_train=np.column_stack((new_train, kurtosis_col))\n",
    "    \n",
    "    lst= [0.01,0.05,0.10,0.25,0.50,0.60, 0.70, 0.80, 0.9, 0.95]\n",
    "    for l in lst:\n",
    "        quintile_col = np.quantile(train,l, axis=1)\n",
    "        new_train=np.column_stack((new_train, med_col))\n",
    "    \n",
    "    total_features= new_train.shape[0]\n",
    "    \n",
    "#     new_train_2 = new_train**2\n",
    "#     new_train_3 = new_train**2\n",
    "    \n",
    "#     new_train=np.column_stack((new_train, new_train_2))\n",
    "#     new_train=np.column_stack((new_train, new_train_3))\n",
    "        \n",
    "    return new_train\n",
    "#Append few extra features/columns\n",
    "def append_bin_features(train):\n",
    "    \n",
    "    min_col = np.min(train, axis=1).reshape((-1,1))\n",
    "    max_col = np.max(train, axis=1).reshape((-1,1))\n",
    "    mean_col = np.mean(train, axis=1).reshape((-1,1))\n",
    "    std_col = np.std(train, axis=1).reshape((-1,1))\n",
    "    sum_col = np.sum(train, axis=1).reshape((-1,1))\n",
    "    med_col = np.median(train, axis=1).reshape((-1,1))\n",
    "    skew_col = scipy.stats.skew(train, axis=1).reshape((-1,1))\n",
    "    kurtosis_col = scipy.stats.kurtosis(train, axis=1).reshape((-1,1))\n",
    "    \n",
    "\n",
    "    new_train = train.copy()\n",
    "    \n",
    "    new_train=np.column_stack((new_train, min_col))\n",
    "    new_train=np.column_stack((new_train, max_col))\n",
    "    new_train=np.column_stack((new_train, mean_col))\n",
    "    new_train=np.column_stack((new_train, std_col))\n",
    "    new_train=np.column_stack((new_train, sum_col))\n",
    "    new_train=np.column_stack((new_train, med_col))\n",
    "    new_train=np.column_stack((new_train, skew_col))\n",
    "    new_train=np.column_stack((new_train, kurtosis_col))\n",
    "    \n",
    "    \n",
    "    lst= [0.01,0.05,0.10,0.25,0.50,0.60, 0.70, 0.80, 0.9, 0.95]\n",
    "    for l in lst:\n",
    "        quintile_col = np.quantile(train,l, axis=1)\n",
    "        new_train=np.column_stack((new_train, med_col))\n",
    "    \n",
    "    total_features= new_train.shape[0]\n",
    "    \n",
    "    \n",
    "    bin_train = train.copy()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(bin_train)\n",
    "    min_max_train = np.round(scaler.transform(bin_train) * 10.0)\n",
    "    \n",
    "    scaler2 = MinMaxScaler()\n",
    "    scaler2.fit(min_max_train)\n",
    "    min_max_train = scaler2.transform(min_max_train)\n",
    "    \n",
    "    new_train=np.column_stack((new_train, min_max_train))\n",
    "    \n",
    "    \n",
    "    return new_train\n",
    "\n",
    "def classify(X_train, X_test, y_train, y_test, nest=25, max_depth=15, min_samples_leaf=80,max_features='auto',class_weight={0:1,1:9}):\n",
    "    classifier=RandomForestClassifier(n_estimators=nest, max_depth=max_depth, min_samples_leaf=min_samples_leaf,max_features=max_features, random_state=42, class_weight=class_weight,n_jobs=-1,verbose=True)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    predictions = classifier.predict(X_train)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, predictions)\n",
    "    roc_auc_tr = auc(false_positive_rate, true_positive_rate)\n",
    "    pred_test = classifier.predict(X_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, pred_test)\n",
    "    roc_auc_test = auc(false_positive_rate, true_positive_rate)\n",
    "    return roc_auc_tr, roc_auc_test, classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape (359804, 200)\n",
      "Test Shape (200000, 200)\n"
     ]
    }
   ],
   "source": [
    "#Reading Input\n",
    "org_train, org_labels, org_test,ids = read_input_balanced()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Test AUC: 0.5000104296312777 0.5000138577090436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#try a basic random forest and see how does it work\n",
    "#Split the data into train and test (validation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(org_train, org_labels, test_size=0.20)\n",
    "tr_auc,test_auc,classifier= classify(X_train, X_test, y_train, y_test, nest =50, class_weight={0:1,1:5})\n",
    "print(\"Training and Test AUC:\", tr_auc, test_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   57.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Test AUC: 0.5 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#Outlier Removal\n",
    "train,labels= remove_outlier(org_train, org_labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.20)\n",
    "tr_auc,test_auc,classifier= classify(X_train, X_test, y_train, y_test,nest=25)\n",
    "print(\"Training and Test AUC:\", tr_auc, test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(342743, 200) (342743,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   43.2s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Test AUC: 0.5 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "#outlier Removal + normalization\n",
    "\n",
    "train,labels= remove_outlier(org_train, org_labels)\n",
    "scaler= normalize(train)\n",
    "train = scaler.transform(train)\n",
    "print(train.shape, labels.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.20)\n",
    "tr_auc,test_auc,classifier= classify(X_train, X_test, y_train, y_test,nest=25)\n",
    "print(\"Training and Test AUC:\", tr_auc, test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188969, 216) (188969,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   25.4s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=12)]: Done  25 out of  25 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=12)]: Using backend ThreadingBackend with 12 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Test AUC: 0.8132638502234497 0.7171743445215831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=12)]: Done  25 out of  25 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#outlier Removal+advanced features without bin + normalization\n",
    "train,labels= remove_outlier(org_train, org_labels)\n",
    "scaler= normalize(train)\n",
    "new_train = append_features(scaler.transform(train))\n",
    "print(new_train.shape, labels.shape)\n",
    "# test= scaler.transform(org_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_train, labels, test_size=0.20)\n",
    "tr_auc,test_auc,classifier= classify(X_train, X_test, y_train, y_test,nest=25)\n",
    "print(\"Training and Test AUC:\", tr_auc, test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(342743, 418) (342743,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   44.9s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Test AUC: 0.5034320269018389 0.502377225024642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "#outlier Removal+advanced features with bin + normalization\n",
    "train,labels= remove_outlier(org_train, org_labels)\n",
    "scaler= normalize(train)\n",
    "new_train = append_bin_features(scaler.transform(train))\n",
    "print(new_train.shape, labels.shape)\n",
    "# test= scaler.transform(org_test)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_train, labels, test_size=0.20)\n",
    "tr_auc,test_auc,classifier= classify(X_train, X_test, y_train, y_test,nest=25)\n",
    "print(\"Training and Test AUC:\", tr_auc, test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(342743, 218) (342743,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:   59.1s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Test AUC: 0.5036308535128362 0.5036308535128362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "#WithoutBin\n",
    "#train,labels= remove_outlier(org_train, org_labels)\n",
    "scaler= normalize(train)\n",
    "new_train = append_features(scaler.transform(train))\n",
    "print(new_train.shape, labels.shape)\n",
    "new_test = append_features(scaler.transform(org_test))\n",
    "\n",
    "tr_auc,test_auc,classifier= classify(new_train, new_train, labels, labels,nest=25)\n",
    "print(\"Training and Test AUC:\", tr_auc, test_auc)\n",
    "\n",
    "\n",
    "test_predictions = classifier.predict(new_test)\n",
    "columns=['ID_code','target']\n",
    "query_output = pd.DataFrame(columns=columns)\n",
    "query_output['ID_code']= ids\n",
    "query_output['target']=test_predictions\n",
    "query_output.to_csv('output_random_forest.csv', index=False,sep=',')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(342743, 418) (342743,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:  3.4min finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=4)]: Done  75 out of  75 | elapsed:    2.4s finished\n",
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done  75 out of  75 | elapsed:    2.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Test AUC: 0.8674572835122331 0.8674572835122331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=4)]: Done  75 out of  75 | elapsed:    1.6s finished\n"
     ]
    }
   ],
   "source": [
    "#WithBin\n",
    "train,labels= remove_outlier(org_train, org_labels)\n",
    "scaler= normalize(np.concatenate((train, org_test), axis=0))\n",
    "new_train = append_bin_features(scaler.transform(train))\n",
    "print(new_train.shape, labels.shape)\n",
    "new_test = append_bin_features(scaler.transform(org_test))\n",
    "\n",
    "tr_auc,test_auc,classifier= classify(new_train, new_train, labels, labels,nest=75,class_weight={0:1,1:1})\n",
    "print(\"Training and Test AUC:\", tr_auc, test_auc)\n",
    "\n",
    "\n",
    "test_predictions = classifier.predict(new_test)\n",
    "columns=['ID_code','target']\n",
    "query_output = pd.DataFrame(columns=columns)\n",
    "query_output['ID_code']= ids\n",
    "query_output['target']=test_predictions\n",
    "query_output.to_csv('output_random_forest.csv', index=False,sep=',')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\thm_m\\\\Desktop\\\\ML\\\\ML'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('C:\\\\Users\\\\thm_m\\\\Desktop\\\\ML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
